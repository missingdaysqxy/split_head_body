{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split head and body(clothes) in masks\n",
    "\n",
    "* Visit [face_segmentation_keras](https://github.com/shaoanlu/face-segmentation-keras) and download [Keras_FCN8s_face_seg_YuvalNirkin.h5](https://drive.google.com/open?id=1alyR6uv4CHt1WhykiQIiK5MZir7HSOUU)\n",
    "* Need to install [maskrcnn_benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# FaceModel must be imported before COCODemo, otherwise \n",
    "# there will be a serious conflict leading to kernel restart\n",
    "from face_fcn8 import FaceModel\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from rcnn import COCODemo as PersonModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU limit for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "tfconfig = tf.ConfigProto()  \n",
    "tfconfig.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "session = tf.Session(config=tfconfig)\n",
    "KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/liuqixuan/datasets/head_test/17618242045124_0.jpg\"\n",
    "rcnn_cfg = \"./rcnn_configs/caffe2/e2e_mask_rcnn_X-152-32x8d-FPN-IN5k_1.44x_caffe2.yaml\"\n",
    "face_weight = \"Keras_FCN8s_face_seg_YuvalNirkin.h5\"\n",
    "noisy_filter_size = 20 # Size of the morphological kernel, the larger the value, the greater the range of filters\n",
    "line_color = [255, 255, 0]\n",
    "line_thickness = 5\n",
    "body_color = [255, 20, 127]\n",
    "head_color = [0, 23, 232]\n",
    "mask_alpha = 0.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Models\n",
    "body_model must be instantiated before face_model, otherwise \n",
    "there may be a serious conflict leading to kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_file(rcnn_cfg)\n",
    "cfg.freeze()\n",
    "person_model = PersonModel(\n",
    "    cfg,\n",
    "    confidence_threshold=0.7,\n",
    "    show_mask_heatmaps=True,\n",
    "    masks_per_dim=2,\n",
    "    min_image_size=224,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model = FaceModel()\n",
    "face_model.load_weights(face_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# visualize\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "print(\"Image size is \", image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Person Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get person mask\n",
    "predictions = person_model.compute_prediction(image)\n",
    "predictions = person_model.select_top_predictions(predictions)\n",
    "masks = predictions.get_field(\"mask\").numpy()\n",
    "labels = predictions.get_field(\"labels\")\n",
    "bboxes = predictions.bbox\n",
    "print(\"%d object(s) detected.\" % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = []  # [offset, person_img, person_mask]\n",
    "canvas1 = np.zeros_like(image) # apply alpha\n",
    "canvas2 = image.copy() # not apply alpha\n",
    "for mask, label, bbox in zip(masks, labels, bboxes):\n",
    "    if label == 1:\n",
    "        mask = mask[0, :, :, np.newaxis]\n",
    "        _, thresh = cv2.threshold(mask, mask.mean(), 255, cv2.THRESH_BINARY)\n",
    "        x1, y1, x2, y2 = bbox.numpy().astype(np.int32)\n",
    "        person_img = image[y1:y2 + 1, x1:x2 + 1]\n",
    "        person_mask = thresh[y1:y2 + 1, x1:x2 + 1]\n",
    "        print(image.shape, person_img.shape, person_mask.shape, (x1, y1), (x2, y2))\n",
    "        persons.append([(x1,y1), person_img, person_mask])\n",
    "        # visualize\n",
    "        blank = np.zeros_like(canvas1, np.uint8)\n",
    "        blank[:] = body_color\n",
    "        canvas1 = cv2.bitwise_or(canvas1, blank, mask=thresh)\n",
    "        cv2.rectangle(canvas2, (x1, y1), (x2, y2), line_color, line_thickness)\n",
    "canvas2 = cv2.scaleAdd(canvas1, mask_alpha, canvas2)\n",
    "plt.figure()\n",
    "plt.imshow(canvas2)\n",
    "print(\"Stored %d person(s) into list\" % len(persons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Face Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_face_mask(img, face_model):\n",
    "    im = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = im.shape[0], im.shape[1]\n",
    "    im = cv2.resize(im, (500, 500))\n",
    "    im = np.array(im, dtype=np.float32)\n",
    "    im = im[:,:,::-1]\n",
    "    im -= np.array((104.00698793,116.66876762,122.67891434))\n",
    "    im = im[np.newaxis,:]\n",
    "    #im = im.transpose((2,0,1))\n",
    "    mask = face_model.predict([im])[0]\n",
    "    mask = cv2.resize(mask, (w, h)).argmax(axis=-1).astype(np.uint8)\n",
    "    _, thresh = cv2.threshold(mask[:,:,np.newaxis], mask.mean(), 255, cv2.THRESH_BINARY)\n",
    "    return thresh\n",
    "\n",
    "humans = [] # store human(s) detected by algorithms, include [offset, cropped_img, body_mask, face_mask]\n",
    "for i, (offset, person_img, person_mask) in enumerate(persons):\n",
    "    h, w = person_mask.shape\n",
    "    in_img = person_img[:w]\n",
    "    face_mask = get_face_mask(in_img, face_model)\n",
    "    if h > w:\n",
    "        face_mask = np.concatenate([face_mask, np.zeros((h - w, w), np.uint8)])\n",
    "    print(face_mask.shape, person_mask.shape)\n",
    "    humans.append([offset, person_img, person_mask, face_mask])\n",
    "    # visualize\n",
    "    canvas = np.zeros_like(person_img)\n",
    "    canvas[:] = head_color\n",
    "    canvas = cv2.bitwise_or(canvas, canvas, mask=face_mask)\n",
    "    canvas = cv2.scaleAdd(canvas, mask_alpha, person_img)\n",
    "    plt.figure()\n",
    "    plt.imshow(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Operations\n",
    "\n",
    "* Take the intersection for _Head_\n",
    "* Take the difference set for _Body_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_open(mask, kernel_size=5):\n",
    "    \"\"\"Removing noise by morphological opening operation (erosion followed by dilation)\"\"\"\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    return cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "def bbox(mask):\n",
    "    \"\"\"Find the bounding box of a mask\"\"\"\n",
    "    axis0 = np.any(mask, axis=0)\n",
    "    axis1 = np.any(mask, axis=1)\n",
    "    x, u = np.where(axis0)[0][[0, -1]]\n",
    "    y, v = np.where(axis1)[0][[0, -1]]\n",
    "    return (x, y), (u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset, person_img, person_mask, face_mask = humans[0]\n",
    "denoised_mask = morph_open(face_mask, noisy_filter_size)\n",
    "assert denoised_mask.any(), \\\n",
    "\"No face can be found from {} to {}, may be you can try to decrease noisy_filter_size\".format(offset, face_mask.shape)\n",
    "(x, y), (u, v) = bbox(denoised_mask)\n",
    "h, w = denoised_mask.shape\n",
    "left_y = np.where(denoised_mask[:,x])[0][0]\n",
    "right_y = np.where(denoised_mask[:,u])[0][0]\n",
    "mid_x = (x + u) // 2\n",
    "# visualize\n",
    "face_bbox = cv2.rectangle(denoised_mask[...,np.newaxis].repeat(3,-1), (x, y), (u, v), line_color, 2, 16)\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(15,5)\n",
    "plt.subplot(131)\n",
    "plt.title(\"Origin Face Mask\")\n",
    "plt.imshow(face_mask)\n",
    "plt.subplot(132)\n",
    "plt.title(\"BBox of Denoised Face Mask\")\n",
    "plt.imshow(face_bbox)\n",
    "plt.subplot(133)\n",
    "plt.title(\"Hair Filled Denoised Face Mask\")\n",
    "visual_mask = denoised_mask.copy()\n",
    "visual_mask[:left_y, :mid_x] = 85\n",
    "visual_mask[:right_y, mid_x:] = 175\n",
    "plt.imshow(visual_mask)\n",
    "print(\"Bounding box is from ({}, {}) to ({}, {})\".format(x, y, u, v))\n",
    "print(\"Areas to be filled are (0, 0) to ({mx},{ly}) and ({mx}, 0) to ({w}, {ry})\".format(mx=mid_x, w=w, ly=left_y, ry=right_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the intersection for Head\n",
    "head_mask = denoised_mask.copy()\n",
    "head_mask[:left_y, :mid_x] = 255\n",
    "head_mask[:right_y, mid_x:] = 255\n",
    "head_mask = person_mask & head_mask\n",
    "# Take the difference set for Body\n",
    "diff_mask = person_mask & (~head_mask)\n",
    "body_mask = morph_open(diff_mask, int(1.3 * noisy_filter_size))\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12,8)\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.title(\"Head Mask\")\n",
    "plt.imshow(head_mask)\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.title(\"Diff Mask\")\n",
    "plt.imshow(diff_mask)\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.title(\"Body Mask\")\n",
    "plt.imshow(body_mask)\n",
    "\n",
    "blank = np.zeros_like(person_img)\n",
    "plt.subplot(234)\n",
    "plt.title(\"Head\")\n",
    "blank[:] = head_color\n",
    "v_head = cv2.scaleAdd(cv2.bitwise_and(blank, blank, mask=head_mask), mask_alpha, person_img.copy())\n",
    "plt.imshow(v_head)\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.title(\"Diff\")\n",
    "blank[:] = body_color\n",
    "v_diff = cv2.scaleAdd(cv2.bitwise_and(blank, blank, mask=diff_mask), mask_alpha, person_img.copy())\n",
    "plt.imshow(v_diff)\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.title(\"Body\")\n",
    "blank[:] = body_color\n",
    "v_body = cv2.scaleAdd(cv2.bitwise_and(blank, blank, mask=body_mask), mask_alpha, person_img.copy())\n",
    "plt.imshow(v_body)\n",
    "\n",
    "print(\"Final head mask and body mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
